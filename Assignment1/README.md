# Multiclass Classification

### Stochastic Gradient Descent from scratch

The notebook "Multiclass Classification using NN with SGD" is my implementation of a 3-layer neural network from scratch for a 3-class classification problem. Here are the specifics:
1. Number of hidden layers = 2
2. Number of nodes in each layer:
    - input layer: 2
    - first hidden layer: 5
    - second hidden layer: 5
    - output layer: 3
3. Activation functions:
    - hidden layers: tanh
    - output layer: softmax
4. Number of epochs: 1000
5. Optimizer: Stochastic Gradient Descent
6. Learning rate: 0.001
7. Loss function: Cross-entropy loss
### Stochastic Gradient Descent from scratch with momentum (Generalised Delta Rule)

The notebook "Multiclass Classification using NN with SGD and MOMENTUM" is my implementation of a 3-layer neural network from scratch for a 3-class classification problem. Here are the specifics:
1. Number of hidden layers = 2
2. Number of nodes in each layer:
    - input layer: 2
    - first hidden layer: 5
    - second hidden layer: 5
    - output layer: 3
3. Activation functions:
    - hidden layers: tanh
    - output layer: softmax
4. Number of epochs: 1000
5. Optimizer: Stochastic Gradient Descent
6. Learning rate: 0.001
7. I have shown results for beta = 0.8 and beta = 0.9
8. Loss function: Cross entropy

# Function Approximation using Neural Networks

Refer notebook titled: "Function Approximation using Neural Networks (SGD with Momentum).ipynb"

Used a 3-layer neural for function approximation.
1. Activation function used: sigmoid
2. Optimizer: SGD with momentum
3. beta: I have shown the results with beta = 0.8 and beta = 0.9
4. learning_rate: I have shown the results for alpha = 0.05 and 0.001

